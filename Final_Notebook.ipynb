{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGNeHgX4-wDg"
   },
   "source": [
    "**Importing the Required** **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6liLDpjW4qE",
    "outputId": "79139074-1fcc-4271-c73c-5f73579204ec"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from scipy.optimize import minimize, curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFJCo5U8-5VE"
   },
   "source": [
    "**Initializing The Datasets and Preprocessing of Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19yqR65gW4qH"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3yhmj5R-qPk"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qEDBVBvW4qI"
   },
   "outputs": [],
   "source": [
    "comments = 'Comment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJylx-ETorJG"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esSo8yWnozWl",
    "outputId": "f159bf1a-112d-48b9-b46b-3bc92ac60a5e"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09cIWKvEouzv"
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ge9UAiUF-m-N"
   },
   "outputs": [],
   "source": [
    "# Making a list containing all the sentences in the comments of the corpus\n",
    "\n",
    "train_sentences = []\n",
    "train_list = []\n",
    "for comment in train_df['Comment']:\n",
    "    sentences = sent_tokenize(comment)\n",
    "    train_list.append(sentences)\n",
    "\n",
    "for data in train_list:\n",
    "    for sent in data:\n",
    "        train_sentences.append(sent)\n",
    "\n",
    "test_sentences = []\n",
    "test_list = []\n",
    "for comment in test_df['Comment']:\n",
    "    sentences = sent_tokenize(comment)\n",
    "    test_list.append(sentences)\n",
    "\n",
    "for data in test_list:\n",
    "    for sent in data:\n",
    "        test_sentences.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEZ7904s-m7j"
   },
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# Preprocessing the sentences to remove unwanted symbols\n",
    "train_sentences_temp=[]\n",
    "for sent in train_sentences:\n",
    "    sentence_without_commas = sent\n",
    "    sentence_without_commas = sentence_without_commas.replace(',', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace(',', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('.', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('!', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('?', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('-', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('\"', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace(')', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('(', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace(']', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('[', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace(\"'\", '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('{', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('}', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('%', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('$', '')\n",
    "    train_sentences_temp.append(sentence_without_commas)\n",
    "\n",
    "train_sentences = train_sentences_temp\n",
    "\n",
    "test_sentences_temp=[]\n",
    "for sent in test_sentences:\n",
    "    sentence_without_commas = sent\n",
    "    sentence_without_commas = sentence_without_commas.replace(',', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace(',', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('.', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('!', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('?', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('-', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('\"', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace(')', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('(', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace(']', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('[', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace(\"'\", '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('{', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('}', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('%', '')\n",
    "    sentence_without_commas = sentence_without_commas.replace('$', '')\n",
    "    test_sentences_temp.append(sentence_without_commas)\n",
    "\n",
    "test_sentences = test_sentences_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9oG1z8TP_0OF",
    "outputId": "bf27d650-938d-4323-d7b1-c1aabad66c2c"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8KEw1CQ_mYH"
   },
   "outputs": [],
   "source": [
    "# Converting the list of sentences to a 2-D list that can be processed by the n-gram modules\n",
    "\n",
    "train_unigram = []\n",
    "for sent in train_sentences:\n",
    "    words = word_tokenize(sent)\n",
    "    proc_words = []\n",
    "    for word in words:\n",
    "        # To make sure that we only include words that contain only alphabets and are also not stop words.\n",
    "        if(word.isalpha() and word not in stops): proc_words.append(lemmatizer.lemmatize(word.lower()))\n",
    "    if len(proc_words)!=0:\n",
    "        train_unigram.append(proc_words)\n",
    "\n",
    "test_unigram = []\n",
    "for sent in test_sentences:\n",
    "    words = word_tokenize(sent)\n",
    "    proc_words = []\n",
    "    for word in words:\n",
    "        if(word.isalpha() and word not in stops): proc_words.append(lemmatizer.lemmatize(word.lower()))\n",
    "    if len(proc_words)!=0:\n",
    "        test_unigram.append(proc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XnDc3c6Ahiw"
   },
   "outputs": [],
   "source": [
    "# The deepcopy method needs to be used instead of the usual copy method since we are working with 2-D arrays\n",
    "train_bigram = copy.deepcopy(train_unigram)\n",
    "test_bigram = copy.deepcopy(test_unigram)\n",
    "train_trigram = copy.deepcopy(train_unigram)\n",
    "test_trigram = copy.deepcopy(test_unigram)\n",
    "train_quadgram = copy.deepcopy(train_unigram)\n",
    "test_quadgram = copy.deepcopy(test_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWUfezwt-m4j"
   },
   "outputs": [],
   "source": [
    "# Adds a single start and stop symbol to the start and end of each sentence in the corpus\n",
    "for sent in train_bigram:\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0, '<s>')\n",
    "for sent in test_bigram:\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0, '<s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5ALtZ2P_7yV"
   },
   "outputs": [],
   "source": [
    "# Adds 2 start and 2 stop symbols to the start and end of each sentence in the corpus\n",
    "for sent in train_trigram:\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0, '<s>')\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0, '<s>')\n",
    "for sent in test_trigram:\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0, '<s>')\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0, '<s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUb5Op34_9ZU"
   },
   "outputs": [],
   "source": [
    "# Adds 3 start and 3 stop symbols to the start and end of each sentence in the corpus\n",
    "for sent in train_quadgram:\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0, '<s>')\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0, '<s>')\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0, '<s>')\n",
    "for sent in test_quadgram:\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0, '<s>')\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0, '<s>')\n",
    "    sent.append('</s>')\n",
    "    sent.insert(0, '<s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59HuRVu2Bbi3"
   },
   "source": [
    "**Defining the n-gram modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hp1-o10C1i6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "start_sent='<s>'\n",
    "end_sent='</s>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CBBO-TOFgtk"
   },
   "source": [
    "**Unigram Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c-RGuEOCqvl"
   },
   "outputs": [],
   "source": [
    "class unigram_model():\n",
    "\n",
    "    def __init__(self, sentences):\n",
    "\n",
    "    # The required data structures are initialized\n",
    "        self.unigram_frequencies = dict()\n",
    "        self.vocabulary = set()\n",
    "        self.corpus_size = 0\n",
    "        self.unigram_count_1 = 0\n",
    "\n",
    "    # The model starts training on the training data\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                self.unigram_frequencies[word] = self.unigram_frequencies.get(word, 0)+1\n",
    "                if word != start_sent or  word != end_sent:\n",
    "                    self.corpus_size+=1\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary.add(word)\n",
    "            \n",
    "        for word in self.unigram_frequencies:\n",
    "            if(self.unigram_frequencies[word]==1):\n",
    "                self.unigram_count_1+=1\n",
    "\n",
    "        self.vocab_size = len(self.unigram_frequencies)-2 # Not including start and end of sentence in vocabulary\n",
    "\n",
    "  # Probability of occurence of a single unigram\n",
    "    def calculate_probability(self,word):\n",
    "        if word not in self.vocabulary:\n",
    "            return 0\n",
    "\n",
    "        return float(self.unigram_frequencies[word])/float(self.corpus_size)\n",
    "\n",
    "  # The smoothed probability using add-k laplace smoothing\n",
    "    def calculate_probability_smooth(self , word, k):\n",
    "        return float(self.unigram_frequencies[word] + float(k))/(float(self.corpus_size) + k*float(self.vocab_size))\n",
    "    \n",
    "    \n",
    "    def calculate_new_word_count(self,diction:dict):\n",
    "    \n",
    "    # freq_to_words_dict : {Key:Number of words occured a particular number of times, Value:List of words that occured a particular number of times}\n",
    "        freq_to_words_dict = dict()\n",
    "\n",
    "        for word in diction.keys():\n",
    "            if(diction[word] in freq_to_words_dict.keys()):\n",
    "                freq_to_words_dict[diction[word]] += [word]\n",
    "            else:\n",
    "                freq_to_words_dict[diction[word]] = [word]\n",
    "\n",
    "    # freq_to_words_dict[0] = [\"<unk>\"]\n",
    "    # new_word_count : {Key:Word, Value:New count of word according to good turing}\n",
    "        x_values = list(freq_to_words_dict.keys())\n",
    "        x_values_2 = []\n",
    "        y_values = []\n",
    "        \n",
    "\n",
    "        for i in x_values:\n",
    "            if(i+1 not in freq_to_words_dict.keys()):\n",
    "                x_values_2.append(i)\n",
    "                y_values.append(len(freq_to_words_dict[i]))\n",
    "\n",
    "        x_values = np.array(x_values_2)\n",
    "        y_values = np.array(y_values)\n",
    "        x_values_3 = x_values.argsort()[:5]\n",
    "        x_values = x_values[x_values_3]\n",
    "        y_values = y_values[x_values_3]\n",
    "\n",
    "\n",
    "        def power_law(x, C, alpha):\n",
    "              return C * np.power(x, -alpha)\n",
    "\n",
    "        params, _ = curve_fit(power_law, x_values, y_values)\n",
    "\n",
    "        new_word_count = dict()\n",
    "        keys = freq_to_words_dict.keys()\n",
    "\n",
    "        for i in freq_to_words_dict.keys():\n",
    "            for word in freq_to_words_dict[i]:\n",
    "\n",
    "                if i + 1 in freq_to_words_dict.keys():\n",
    "                    new_word_count[word] = (i + 1) * (\n",
    "                    len(freq_to_words_dict[i + 1]) / len(freq_to_words_dict[i]))\n",
    "\n",
    "                else:\n",
    "                    new_word_count[word] = (i + 1) * (\n",
    "                    power_law(float(i + 1), params[0], params[1])\n",
    "                    / power_law(float(i), params[0], params[1]))\n",
    "                \n",
    "\n",
    "        new_word_count[\"<unk>\"] = len(freq_to_words_dict[1]) / self.corpus_size\n",
    "        self.good_turing_count = new_word_count\n",
    "\n",
    "    def calculate_probability_good_smooth(self, word):\n",
    "\n",
    "        new_word_count = self.good_turing_count\n",
    "        if word not in new_word_count:\n",
    "            return new_word_count[\"<unk>\"]\n",
    "\n",
    "        return new_word_count[word] / self.corpus_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ4H0OWZFmYK"
   },
   "source": [
    "#### Bigram Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNi9Qz5IEBoE"
   },
   "outputs": [],
   "source": [
    "class bigram_model(unigram_model):\n",
    "\n",
    "    def __init__(self, sentences):\n",
    "\n",
    "        unigram_model.__init__(self, sentences) # Constructer of the parent class is run to get information about the unigrams\n",
    "\n",
    "        # The required data structures are initialized\n",
    "        self.bigram_frequencies=dict()\n",
    "        self.total_bigrams=0\n",
    "        self.second_word_app=dict()\n",
    "        self.first_word_app=dict()\n",
    "        self.bigram_count_1=0\n",
    "\n",
    "        # The model starts training on the training data\n",
    "        for sentence in sentences:\n",
    "            prev_word = sentence[0]\n",
    "            for word in sentence[1:]:\n",
    "                self.bigram_frequencies[(prev_word, word)] = self.bigram_frequencies.get((prev_word, word), 0)+1\n",
    "                self.total_bigrams += 1\n",
    "                prev_word = word\n",
    "\n",
    "        for bigram in self.bigram_frequencies:\n",
    "            if(self.bigram_frequencies[bigram]==1):\n",
    "                self.bigram_count_1 += 1\n",
    "\n",
    "        self.total_bigram_words = len(self.bigram_frequencies)\n",
    "\n",
    "    def calculate_probability(self, prev_word, word):\n",
    "\n",
    "        a = self.bigram_frequencies.get((prev_word, word), 0)\n",
    "        b = self.unigram_frequencies.get(prev_word, 0)\n",
    "\n",
    "        if b == 0:\n",
    "            return 0\n",
    "\n",
    "        return float(a)/float(b)\n",
    "\n",
    "  # The smoothed probability using add-k laplace smoothing\n",
    "    def calculate_probability_smooth(self, prev_word,word , k, k_prev):\n",
    "\n",
    "        a = self.bigram_frequencies.get((prev_word, word),0)\n",
    "        b = self.unigram_frequencies.get(prev_word, 0)\n",
    "\n",
    "\n",
    "        return (float(a) + float(k)) /(float(b)+ float(k_prev) + k*self.vocab_size) # The best k obtained for unigrams is used here to better represent the effective count of unigrams\n",
    "\n",
    "  # Running this to get the a dictionary which tells us how many unique bigrams does any given word complete\n",
    "    def calculate_self_second_word(self,):\n",
    "        for key in self.bigram_frequencies:\n",
    "            self.second_word_app[key[1]] = self.second_word_app.get(key[1], 0) + 1\n",
    "\n",
    "  # Running this to get the a dictionary which tells us how many unique bigrams does any given word generate\n",
    "    def calculate_self_first_word(self,):\n",
    "        for key in self.bigram_frequencies:\n",
    "            self.first_word_app[key[0]] = self.first_word_app.get(key[1], 0) + 1\n",
    "\n",
    "    def initialise_kneser_ney(self):\n",
    "        self.calculate_self_second_word()\n",
    "        self.calculate_self_first_word()\n",
    "\n",
    "  # To calculate the continuation probability for a word\n",
    "    def calculate_continuation_probability(self, word):\n",
    "\n",
    "        if(word in self.second_word_app):\n",
    "            return float(self.second_word_app[word]) / float(len(self.bigram_frequencies))\n",
    "        else:\n",
    "            return 0;\n",
    "\n",
    "    def kneser_ney_smoothing(self, prev_word, word, d = 0.75):\n",
    "    \n",
    "      # d is the discounting factor, usually taken to be 0.75\n",
    "        if((prev_word, word) in self.bigram_frequencies.keys()):\n",
    "            term1 = max(self.bigram_frequencies[(prev_word, word)] - d, 0)\n",
    "            term1 /= self.unigram_frequencies[prev_word]\n",
    "            lambda_val = d / (self.unigram_frequencies[prev_word])\n",
    "            lambda_val *= self.first_word_app[prev_word]\n",
    "            term_2 = lambda_val * self.calculate_continuation_probability(word)\n",
    "\n",
    "        else:\n",
    "            if(prev_word in self.unigram_frequencies.keys()):\n",
    "                if(word in self.unigram_frequencies.keys()):\n",
    "                    term1 = max(self.unigram_frequencies[word] - d, 0)\n",
    "                    term1 /= self.corpus_size\n",
    "                    lambda_val = d / (self.unigram_frequencies[prev_word])\n",
    "                    lambda_val *= self.first_word_app[prev_word]\n",
    "                    term_2 = lambda_val * self.calculate_continuation_probability(word)\n",
    "                else:\n",
    "                    term1 = d/self.corpus_size\n",
    "                    term_2 = 0\n",
    "\n",
    "            else:\n",
    "                term1 = max(self.bigram_count_1 - d,0)/(self.unigram_count_1)\n",
    "                term_2 = 0\n",
    "        return term1 + term_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RysI60fSFqaG"
   },
   "source": [
    "**Trigram Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UstDL4JkE3Ap"
   },
   "outputs": [],
   "source": [
    "class trigram_model(bigram_model):\n",
    "\n",
    "    def __init__(self, sentences):\n",
    "\n",
    "        bigram_model.__init__(self, sentences) # Calling the parent class constructor to get the required count of the bigrams\n",
    "\n",
    "        # Initializing the required data structures\n",
    "        self.trigram_frequencies = {}\n",
    "        self.total_trigrams=0\n",
    "\n",
    "        #The model starts training\n",
    "        for sentence in sentences:\n",
    "            prev_word1 = sentence[0]\n",
    "            prev_word2 = sentence[1]\n",
    "            for word in sentence[2:]:\n",
    "              self.trigram_frequencies[(prev_word1, prev_word2, word)] = self.trigram_frequencies.get((prev_word1, prev_word2, word), 0) + 1\n",
    "              prev_word1 = prev_word2\n",
    "              prev_word2 = word\n",
    "              self.total_trigrams += 1\n",
    "\n",
    "        self.total_trigram_words = len(self.trigram_frequencies)\n",
    "\n",
    "    def calculate_probability(self, prev_word1, prev_word2, word):\n",
    "\n",
    "        trigram_frequency = self.trigram_frequencies.get((prev_word1, prev_word2, word), 0)\n",
    "        bigram_frequency = self.bigram_frequencies.get((prev_word2, word), 0)\n",
    "\n",
    "        if bigram_frequency == 0:\n",
    "            return 0\n",
    "\n",
    "        return float(trigram_frequency) / float(bigram_frequency)\n",
    "\n",
    "    def calculate_probability_smooth(self, prev_word1, prev_word2, word , k, k_prev):\n",
    "\n",
    "        trigram_frequency = self.trigram_frequencies.get((prev_word1, prev_word2, word), 0)\n",
    "        bigram_frequency = self.bigram_frequencies.get((prev_word2, word), 0)\n",
    "\n",
    "        return (float(trigram_frequency) + float(k)) / (float(bigram_frequency)+ float(k_prev) + k*self.vocab_size) #Again, the best k obtained for bigrams is also used to use the effective count of bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sqfZ95IFwhU"
   },
   "source": [
    "**Quadgram Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLT5bMeWW4qM"
   },
   "outputs": [],
   "source": [
    "class quadgram_model(trigram_model):\n",
    "    def __init__(self, sentences):\n",
    "\n",
    "        trigram_model.__init__(self, sentences) # Constructer of the parent class is run to get the required count of the trigrams\n",
    "\n",
    "        # Required data structures are initialized\n",
    "        self.quadgram_frequencies = {}\n",
    "        self.total_quadgrams = 0\n",
    "\n",
    "        # The model starts training\n",
    "        for sentence in sentences:\n",
    "            prev_word1 = sentence[0]\n",
    "            prev_word2 = sentence[1]\n",
    "            prev_word3 = sentence[2]\n",
    "            for word in sentence[3:]:\n",
    "                quadgram = (prev_word1, prev_word2, prev_word3, word)\n",
    "                self.quadgram_frequencies[quadgram] = self.quadgram_frequencies.get(quadgram, 0) + 1\n",
    "                prev_word1 = prev_word2\n",
    "                prev_word2 = prev_word3\n",
    "                prev_word3 = word\n",
    "                self.total_quadgrams += 1\n",
    "\n",
    "        self.total_quadgram_words = len(self.quadgram_frequencies)\n",
    "\n",
    "\n",
    "    def calculate_probability(self, prev_word1, prev_word2, prev_word3, word):\n",
    "        quadgram_frequency = self.quadgram_frequencies.get((prev_word1, prev_word2, prev_word3, word), 0)\n",
    "        trigram_frequency = self.trigram_frequencies.get((prev_word1, prev_word2, prev_word3), 0)\n",
    "\n",
    "        if trigram_frequency == 0:\n",
    "            return 0\n",
    "\n",
    "        return float(quadgram_frequency) / float(trigram_frequency)\n",
    "\n",
    "    def calculate_probability_smooth(self, prev_word1, prev_word2, prev_word3, word , k, k_prev):\n",
    "\n",
    "        quadgram_frequency = self.quadgram_frequencies.get( (prev_word1, prev_word2, prev_word3, word), 0)\n",
    "        trigram_frequency = self.trigram_frequencies.get( (prev_word1, prev_word2, prev_word3), 0)\n",
    "\n",
    "        return (float(quadgram_frequency) + float(k)) / (float(trigram_frequency)+ float(k_prev) + k*self.vocab_size)# The best k for trigrams is used to represent their effective count in the expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJqGJ9FnHHoP"
   },
   "source": [
    "**Perplexities Without Smoothing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHqDd-fsSUuP"
   },
   "source": [
    "**Unigram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4nLD2UELW4qP",
    "outputId": "65406eae-86d2-459c-b5f5-71217830b54e"
   },
   "outputs": [],
   "source": [
    "UNIGRAM_MODEL = unigram_model(train_unigram)\n",
    "\n",
    "count = 0\n",
    "total_perplex = 0\n",
    "for data in test_unigram:\n",
    "\n",
    "    count+=1\n",
    "    n = len(data)\n",
    "    if(n == 0): continue\n",
    "    unigram_perplex=0\n",
    "    for word in data:\n",
    "        unigram_perplex += np.log2(UNIGRAM_MODEL.calculate_probability(word))\n",
    "\n",
    "    unigram_perplex *= (-1/n)\n",
    "    final_perplex = 2 ** unigram_perplex\n",
    "    total_perplex += final_perplex\n",
    "\n",
    "avg_perplex = total_perplex/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMFOB-pZW4qP",
    "outputId": "6a5f702e-ee17-44c2-aae7-9311e699e2a3"
   },
   "outputs": [],
   "source": [
    "avg_perplex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbZG_e9BSXUH"
   },
   "source": [
    "**Bigram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cRuczKsW4qQ",
    "outputId": "aa338ac4-38d9-4448-8633-ece044a31a58"
   },
   "outputs": [],
   "source": [
    "BIGRAM_MODEL = bigram_model(train_bigram)\n",
    "total_perplex = 0\n",
    "count = 0\n",
    "\n",
    "for data in test_bigram:\n",
    "\n",
    "    count+= 1\n",
    "    cross_entropy = 0\n",
    "    n = len(data)\n",
    "    if(n < 2): continue\n",
    "\n",
    "    for i in range(len(data)-1):\n",
    "        cross_entropy += np.log2(BIGRAM_MODEL.calculate_probability(data[i], data[i+1]))\n",
    "\n",
    "    cross_entropy *= (-1/n)\n",
    "    final_perplex = 2 ** cross_entropy\n",
    "    total_perplex += final_perplex\n",
    "\n",
    "\n",
    "avg_perplex = total_perplex/count\n",
    "avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7n4GlTtyW4qQ",
    "outputId": "8773150f-1fe5-44fc-8daa-b8a156181335"
   },
   "outputs": [],
   "source": [
    "TRIGRAM_MODEL=trigram_model(train_trigram)\n",
    "total_perplex = 0\n",
    "count = 0\n",
    "\n",
    "for data in test_trigram:\n",
    "    count+= 1\n",
    "    cross_entropy = 0\n",
    "    n = len(data)\n",
    "    if(n < 3): continue\n",
    "\n",
    "    for i in range(len(data)-2):\n",
    "        cross_entropy += np.log2(TRIGRAM_MODEL.calculate_probability(data[i], data[i+1], data[i+2]))\n",
    "\n",
    "    cross_entropy *= (-1/n)\n",
    "    final_perplex = 2 ** cross_entropy\n",
    "    total_perplex += final_perplex\n",
    "\n",
    "\n",
    "avg_perplex = total_perplex/count\n",
    "avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paZDwM3bW4qR",
    "outputId": "ff6af686-fe3d-4950-97fd-5f52f51277f9"
   },
   "outputs": [],
   "source": [
    "QUADGRAM_MODEL = quadgram_model(train_quadgram)\n",
    "total_perplex = 0\n",
    "count = 0\n",
    "\n",
    "for data in test_quadgram:\n",
    "    count += 1\n",
    "    cross_entropy = 0\n",
    "    n = len(data)\n",
    "    if(n < 4): continue\n",
    "\n",
    "    for i in range(len(data)-3):\n",
    "        cross_entropy += np.log2(QUADGRAM_MODEL.calculate_probability(data[i],data[i+1], data[i+2] , data[i+3]))\n",
    "\n",
    "    cross_entropy *= (-1/n)\n",
    "    final_perplex = 2 ** cross_entropy\n",
    "    total_perplex += final_perplex\n",
    "\n",
    "\n",
    "avg_perplex = total_perplex/count\n",
    "avg_perplex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwCT93ysHl7K"
   },
   "source": [
    "**Perplexities With Smoothing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8lkFXoKJoAU"
   },
   "source": [
    "**Unigram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4KZvxNcW4qR"
   },
   "outputs": [],
   "source": [
    "UNIGRAM_MODEL = unigram_model(train_unigram)\n",
    "\n",
    "# Calculating the effective vocabulary size\n",
    "for data in test_unigram:\n",
    "    for word in data:\n",
    "        if word not in UNIGRAM_MODEL.vocabulary:\n",
    "            UNIGRAM_MODEL.vocabulary.add(word)\n",
    "            UNIGRAM_MODEL.vocab_size += 1\n",
    "            UNIGRAM_MODEL.unigram_frequencies[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNCXme88W4qS"
   },
   "outputs": [],
   "source": [
    "# Defining the perplexity as a function of k so that it can be optimized\n",
    "def uni(k):\n",
    "\n",
    "    count = 0 # The total number of test sentences\n",
    "    total_perplex = 0\n",
    "\n",
    "    for data in test_unigram:\n",
    "        count+=1\n",
    "        n = len(data) # Length of the sentence\n",
    "        if(n == 0): continue\n",
    "        cross_entropy = 0\n",
    "        for word in data:\n",
    "            cross_entropy += np.log2(UNIGRAM_MODEL.calculate_probability_smooth(word , k))\n",
    "\n",
    "        # Calculating perplexity as 2^(cross entropy) to provide numerical stability\n",
    "        cross_entropy *= (-1/float(n))\n",
    "        final_perplex = 2 ** cross_entropy\n",
    "        total_perplex += final_perplex\n",
    "\n",
    "    avg_perplex = total_perplex/float(count)\n",
    "    return avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbJ4iXEfW4qS"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svKQNfgOH4XH"
   },
   "outputs": [],
   "source": [
    "k_uni, k_bi, k_tri, k_quad = 0, 0, 0, 0 # Initializing the best k for each n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muat3-YXW4qS"
   },
   "outputs": [],
   "source": [
    "def callback(xk):\n",
    "    print(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xieYThKFW4qT",
    "outputId": "9eb443af-7ea9-4210-9cf2-18478acea82c"
   },
   "outputs": [],
   "source": [
    "result = minimize(uni , 1 , tol = 1e-5 , callback = callback, bounds = [(1 , 15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EdlUg43Ocr8"
   },
   "outputs": [],
   "source": [
    "k_uni = 12.53008756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KiakuHijdXdN",
    "outputId": "384dadea-42cd-484d-e221-27d2735ba5ca"
   },
   "outputs": [],
   "source": [
    "uni(k_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7KUa0F6JsnV"
   },
   "source": [
    "**Bigram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJyius6UXkGn"
   },
   "outputs": [],
   "source": [
    "BIGRAM_MODEL = bigram_model(train_bigram)\n",
    "\n",
    "#Calculating the effective vocabulary count and inserting un-seen bigrams into the dictionary\n",
    "for data in test_bigram:\n",
    "    n = len(data)\n",
    "    for word in data:\n",
    "        if word not in BIGRAM_MODEL.vocabulary:\n",
    "            BIGRAM_MODEL.vocabulary.add(word)\n",
    "            BIGRAM_MODEL.vocab_size += 1\n",
    "\n",
    "    for i in range(n-1):\n",
    "\n",
    "        if(data[i], data[i+1]) not in BIGRAM_MODEL.bigram_frequencies:\n",
    "            BIGRAM_MODEL.bigram_frequencies[(data[i], data[i+1])] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbZ_9WSKW4qV"
   },
   "outputs": [],
   "source": [
    "# Defining the perplexity as a function of k so that it can be optimized\n",
    "def bi(k):\n",
    "\n",
    "    total_perplex = 0\n",
    "    count = 0\n",
    "\n",
    "    for data in test_bigram:\n",
    "        count += 1\n",
    "        cross_entropy = 0\n",
    "        n = len(data)\n",
    "        if(n < 2): continue\n",
    "\n",
    "        for i in range(len(data)-1):\n",
    "\n",
    "            cross_entropy += np.log2(BIGRAM_MODEL.calculate_probability_smooth(data[i], data[i+1] , k , k_uni ))\n",
    "\n",
    "        # Calculating perplexity as 2^(cross entropy) to provide numerical stability\n",
    "        cross_entropy *= (-1/n)\n",
    "        final_perplex = 2 ** cross_entropy\n",
    "        total_perplex += final_perplex\n",
    "\n",
    "\n",
    "    avg_perplex = total_perplex/count\n",
    "    return avg_perplex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram Using Kneser-Ney Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRy3qTwkSU7m",
    "outputId": "2b722f18-1137-4944-c1b1-7f25daf021c8"
   },
   "outputs": [],
   "source": [
    "BIGRAM_MODEL = bigram_model(train_bigram)\n",
    "total_perplex = 0\n",
    "BIGRAM_MODEL.initialise_kneser_ney()\n",
    "count = 0\n",
    "\n",
    "for data in test_bigram:\n",
    "    \n",
    "    count+= 1\n",
    "    cross_entropy = 0\n",
    "    n = len(data)\n",
    "    if(n < 2): continue\n",
    "\n",
    "    for i in range(len(data)-1):\n",
    "        cross_entropy += np.log2(BIGRAM_MODEL.kneser_ney_smoothing(data[i],data[i+1]))\n",
    "\n",
    "    cross_entropy *= (-1/n)\n",
    "    final_perplex = 2 ** cross_entropy\n",
    "    total_perplex += final_perplex\n",
    "\n",
    "\n",
    "avg_perplex = total_perplex/count\n",
    "avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "8syjTUAmXuH-",
    "outputId": "5fefeac6-6421-4b83-ad58-59a58b53b4f0"
   },
   "outputs": [],
   "source": [
    "result = minimize(bi , 1e-8 , tol = 1e-5 , callback = callback , bounds = [(1e-8 , 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihW_qL7yPomd"
   },
   "outputs": [],
   "source": [
    "k_bi = 0.01292838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWBq5tN_dujG",
    "outputId": "05918910-81df-4cd6-f10a-9167a1edbeaa"
   },
   "outputs": [],
   "source": [
    "bi(k_bi) # Final optimized perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwH4GftWKyWp"
   },
   "source": [
    "**Trigram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sIW5UXBW4qW"
   },
   "outputs": [],
   "source": [
    "TRIGRAM_MODEL = trigram_model(train_trigram)\n",
    "\n",
    "for data in test_trigram:\n",
    "    n = len(data)\n",
    "    for word in data:\n",
    "        if word not in TRIGRAM_MODEL.vocabulary:\n",
    "            TRIGRAM_MODEL.vocabulary.add(word)\n",
    "            TRIGRAM_MODEL.vocab_size += 1\n",
    "\n",
    "    for i in range(n-2):\n",
    "        if(data[i], data[i+1], data[i+2]) not in TRIGRAM_MODEL.trigram_frequencies:\n",
    "            TRIGRAM_MODEL.trigram_frequencies[(data[i], data[i+1] , data[i+2])] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30VV0YzDW4qW"
   },
   "outputs": [],
   "source": [
    "# Defining the perplexity as a function so that it can be optimized\n",
    "def tri(k):\n",
    "\n",
    "    total_perplex = 0\n",
    "    count = 0\n",
    "\n",
    "    for data in test_trigram:\n",
    "        count+= 1\n",
    "        cross_entropy = 0\n",
    "        n = len(data)\n",
    "        if(n < 3): continue\n",
    "\n",
    "        for i in range(len(data)-2):\n",
    "            cross_entropy += np.log2(TRIGRAM_MODEL.calculate_probability_smooth(data[i],data[i+1], data[i+2] ,k , k_bi))\n",
    "\n",
    "        # Calculating perplexity as 2^(cross entropy) to provide numerical stability\n",
    "        cross_entropy *= (-1/n)\n",
    "        final_perplex = 2 ** cross_entropy\n",
    "        total_perplex += final_perplex\n",
    "\n",
    "\n",
    "    avg_perplex=total_perplex/count\n",
    "    return avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEoGAEoEYEQ9"
   },
   "outputs": [],
   "source": [
    "result = minimize(tri , 0.001 , tol = 1e-5 , callback = callback , bounds = [(0.0001 , 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3tfTgfmQzhN"
   },
   "outputs": [],
   "source": [
    "k_tri = 0.00114886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKBWnv-mgUlC",
    "outputId": "f58926ff-842a-4dfc-fe17-e5462356b179"
   },
   "outputs": [],
   "source": [
    "tri(k_tri) #Final optimized perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AqEEDiILOC3"
   },
   "source": [
    "**Quadgram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIBjqa8VW4qX"
   },
   "outputs": [],
   "source": [
    "QUADGRAM_MODEL= quadgram_model(train_quadgram)\n",
    "\n",
    "for data in test_quadgram:\n",
    "    n = len(data)\n",
    "    for word in data:\n",
    "        if word not in QUADGRAM_MODEL.vocabulary:\n",
    "            QUADGRAM_MODEL.vocabulary.add(word)\n",
    "            QUADGRAM_MODEL.vocab_size += 1\n",
    "\n",
    "    for i in range(n-3):\n",
    "        if(data[i], data[i+1], data[i+2] , data[i+3]) not in QUADGRAM_MODEL.quadgram_frequencies:\n",
    "            QUADGRAM_MODEL.quadgram_frequencies[(data[i], data[i+1] , data[i+2] , data[i+3])] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUxeWvd7W4qY"
   },
   "outputs": [],
   "source": [
    "def quad(k):\n",
    "\n",
    "    total_perplex = 0\n",
    "    count = 0\n",
    "\n",
    "    for data in test_quadgram:\n",
    "        count += 1\n",
    "        cross_entropy = 0\n",
    "        n = len(data)\n",
    "        if(n < 4): continue\n",
    "\n",
    "        for i in range(len(data)-3):\n",
    "            prob = QUADGRAM_MODEL.calculate_probability_smooth(data[i], data[i+1], data[i+2],data[i+3] , k, k_tri)\n",
    "            cross_entropy += np.log2(prob)\n",
    "\n",
    "        cross_entropy *= (-1/n)\n",
    "        final_perplex = 2 ** cross_entropy\n",
    "        total_perplex += final_perplex\n",
    "\n",
    "\n",
    "    avg_perplex = total_perplex/count\n",
    "    return avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FsCjuyJYWKf"
   },
   "outputs": [],
   "source": [
    "result = minimize(quad , 0.001 , tol = 1e-5 , callback = callback , bounds = [(0.00001 , 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IOO1VQmRaKU"
   },
   "outputs": [],
   "source": [
    "k_quad = 0.00040418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UFUotK3RiQXD",
    "outputId": "3e10c14b-2caa-4493-8c92-04df9679b913"
   },
   "outputs": [],
   "source": [
    "quad(k_quad) # Final optimized perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6IBVZkInO-p"
   },
   "source": [
    "##### Perplexities using Good-Turing smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIGRAM_MODEL = unigram_model(train_unigram)\n",
    "\n",
    "count = 0\n",
    "total_perplex = 0\n",
    "UNIGRAM_MODEL.calculate_new_word_count(UNIGRAM_MODEL.unigram_frequencies)\n",
    "\n",
    "for data in test_unigram:\n",
    "\n",
    "    count+=1\n",
    "    n = len(data)\n",
    "    if(n == 0): continue\n",
    "    cross_entropy = 0\n",
    "    for word in data:\n",
    "        cross_entropy += np.log2(UNIGRAM_MODEL.calculate_probability_good_smooth(word))\n",
    "\n",
    "    cross_entropy *= (-1/n)\n",
    "    final_perplex = 2 ** cross_entropy\n",
    "    total_perplex += final_perplex\n",
    "\n",
    "\n",
    "avg_perplex = total_perplex/count\n",
    "avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIGRAM_MODEL = bigram_model(train_bigram)\n",
    "total_perplex = 0\n",
    "count = 0\n",
    "BIGRAM_MODEL.calculate_new_word_count(BIGRAM_MODEL.bigram_frequencies)\n",
    "\n",
    "for data in test_bigram:\n",
    "    count += 1\n",
    "    cross_entropy = 0\n",
    "    n = len(data)\n",
    "    if(n < 2): continue\n",
    "\n",
    "    for i in range(len(data)-1):\n",
    "        cross_entropy += np.log2(BIGRAM_MODEL.calculate_probability_good_smooth((data[i], data[i+1])))\n",
    "\n",
    "    cross_entropy *= (-1/n)\n",
    "    final_perplex = 2 ** cross_entropy\n",
    "    total_perplex += final_perplex\n",
    "\n",
    "\n",
    "avg_perplex=total_perplex/count\n",
    "avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIGRAM_MODEL = trigram_model(train_trigram)\n",
    "total_perplex = 0\n",
    "count = 0\n",
    "TRIGRAM_MODEL.calculate_new_word_count(TRIGRAM_MODEL.trigram_frequencies)\n",
    "\n",
    "for data in test_trigram:\n",
    "    count += 1\n",
    "    cross_entropy = 0\n",
    "    n = len(data)\n",
    "    if(n < 3): continue\n",
    "\n",
    "    for i in range(len(data)-2):\n",
    "        cross_entropy += np.log2(TRIGRAM_MODEL.calculate_probability_good_smooth((data[i], data[i+1], data[i+2])))\n",
    "\n",
    "    cross_entropy *= (-1/n)\n",
    "    final_perplex = 2 ** cross_entropy\n",
    "    total_perplex += final_perplex\n",
    "\n",
    "\n",
    "avg_perplex = total_perplex/count\n",
    "avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUADGRAM_MODEL = quadgram_model(train_quadgram)\n",
    "total_perplex = 0\n",
    "count = 0\n",
    "QUADGRAM_MODEL.calculate_new_word_count(QUADGRAM_MODEL.quadgram_frequencies)\n",
    "\n",
    "for data in test_quadgram:\n",
    "    count+= 1\n",
    "    cross_entropy = 0\n",
    "    n = len(data)\n",
    "    if(n < 4): continue\n",
    "\n",
    "    for i in range(len(data)-3):\n",
    "        cross_entropy += np.log2(QUADGRAM_MODEL.calculate_probability_good_smooth((data[i], data[i+1], data[i+2] , data[i+3])))\n",
    "\n",
    "    cross_entropy *= (-1/n)\n",
    "    final_perplex = 2 ** cross_entropy\n",
    "    total_perplex += final_perplex\n",
    "\n",
    "\n",
    "avg_perplex = total_perplex/count\n",
    "avg_perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
